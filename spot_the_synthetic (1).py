# -*- coding: utf-8 -*-
"""Spot-the-Synthetic

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r0Bh61KeTf31WWkHvrScLGLt4U5-com2

# Spot-the-synthetic

## Importing Libraries
"""

import os
import re
import zipfile
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image

"""## Extracting the files

#### Extracting training data
"""

zip_file_path = r'/content/training.zip'

# Directory to extract the contents
extract_dir = r"C:\Users\nellu\Downloads\training"

# Create extraction directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Extract the zip file
with zipfile.ZipFile(r"training.zip", 'r') as zip_ref:
    zip_ref.extractall(r"training")

print("Extraction complete.")

"""#### Extracting Testing_data"""

# Path to the zip file
zip_file_path = r'/content/test.zip'

# Directory to extract the contents
extract_dir = r"C:\Users\nellu\Downloads\test"

# Create extraction directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Extract the zip file
with zipfile.ZipFile(r"test.zip", 'r') as zip_ref:
    zip_ref.extractall(r"test")

print("Extraction complete.")

"""##1.Data Preprocessing

### Train_data Preprocessing
"""

#rescaling the image and performing image augemantation by shearing ,zooming in and horizontal flipping
train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,
                                 zoom_range=0.2, horizontal_flip=True,vertical_flip=True,rotation_range=30)
#making the size of image 64x64 and dividing the images in batch of 32 and converting the each pixels into 0's or 1's


training_set=train_datagen.flow_from_directory('/content/training/training',
                                                target_size=(224,224),
                                                batch_size=32,
                                                class_mode="binary")

training_set.class_indices

"""### Test_Data Preprocessing"""

test_datagen=ImageDataGenerator(rescale=1./255)
testing_set=train_datagen.flow_from_directory('/content/test/test',
                                               target_size=(224,224),
                                               batch_size=32,
                                               class_mode="binary")

"""#### keeping images according to their names"""

# Assuming test_dir is the path to your testing directory
test_dir = '/content/test/test/Test_Images'

# Get the list of image filenames in the testing directory
image_filenames = os.listdir(test_dir)

# Extract the numerical part from the image filenames
numbers = [int(re.search(r'\d+', filename).group()) for filename in image_filenames]

# Sort the image filenames and labels based on the extracted numbers
sorted_data = sorted(zip(numbers, image_filenames))

# Unzip the sorted data
sorted_numbers, sorted_filenames = zip(*sorted_data)

print(sorted_filenames)

"""## 2.Importing the pre-trained model MobileNet"""

from keras.applications import MobileNet
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.optimizers import Adam

# Load pre-trained MobileNet model
base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze MobileNet layers
for layer in base_model.layers:
    layer.trainable = False

# Add new classifier layers
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(2, activation='softmax')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=output)

# Compile the model
model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""## 3.Train the model"""

history = model.fit(training_set, epochs=10, validation_data=testing_set)

# Assuming `history` is the object returned by model.fit
# It contains the loss and other metrics for each epoch
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(1, len(accuracy) + 1)

# Plotting the loss for training and validation data
plt.figure()
plt.plot(epochs, accuracy, color='blue', label='Training accuracy')
plt.plot(epochs, val_accuracy, color='red', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('accuracy')
plt.legend()
plt.show()

model.evaluate(training_set)

model.evaluate(testing_set)

model.summary()

"""## 4.Predictions of Testing Data"""

# List to store the predicted classes
predicted_classes = []

# Iterate over the list of image paths
for img_path in sorted_filenames:
    preprocessed_img = preprocess_image(f'/content/test/test/Test_Images/{img_path}')
    predictions = model.predict(preprocessed_img)
    predicted_class = np.argmax(predictions, axis=1)[0]
    predicted_classes.append(predicted_class)

# Display the predicted classes
print(sorted_filenames)
print(predicted_classes)

true_labels = testing_set.classes

# Calculate accuracy
accuracy = accuracy_score(true_labels, predicted_labels)
print("Accuracy:", accuracy)

"""## 5.Making CSV file"""

sorted_filenames=list(sorted_filenames)
import csv
# Zip the lists together
data = zip(sorted_filenames, predicted_labels)

# Specify the file name
file_name = 'submission.csv'

# Write the data to a CSV file
with open(file_name, mode='w', newline='') as file:
    writer = csv.writer(file)
    # Write the header
    writer.writerow(['Column1', 'Column2'])
    # Write the data rows
    for row in data:
        writer.writerow(row)

print(f"CSV file '{file_name}' has been created.")